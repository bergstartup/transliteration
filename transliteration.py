# -*- coding: utf-8 -*-
"""transliteration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AMSg2kwHVXXJYXrRzfQOcYVWaRAsPsvZ
"""

import torch
import torch.nn as nn

SOS=0
EOS=1
class letters:
  def __init__(self,name):
    self.name=name
    self.letter2index={"SOS":0,"EOS":1}
    self.index2letter={0:"SOS",1:"EOS"}
    self.letter_count=2
  
  def fill(self,unicode_range):
    for i in range(unicode_range[0],unicode_range[1]+1):
      self.letter2index[chr(i)]=self.letter_count
      self.index2letter[self.letter_count]=chr(i)
      self.letter_count+=1

def word_tensors(word1,lang1):
  vector1=torch.zeros(len(word1),1,lang1.letter_count,dtype=torch.float)
  for i,j in enumerate(word1):
    vector1[i,0,lang1.letter2index[j]]=1
  return vector1

class TransliterationAttention(nn.Module):
  def __init__(self,input_size,hidden_size,output_size):
    super().__init__()
    self.input_size=input_size
    self.hidden_size=hidden_size
    self.output_size=output_size

    self.encoder_rnn=nn.GRU(input_size,hidden_size)
    self.decoder_rnn=nn.GRU(2*hidden_size,hidden_size)

    self.h2o=nn.Linear(hidden_size,output_size)
    self.sm=nn.LogSoftmax(dim=2)

    self.U=nn.Linear(hidden_size,hidden_size)
    self.W=nn.Linear(hidden_size,hidden_size)
    self.atten=nn.Linear(hidden_size,1)
    self.embedding=nn.Linear(output_size,hidden_size)

  def forward(self,input_,max_output=30,device="cpu",assisted=None):
    #input_ size [word_len,1,input_size]
    all_,final=self.encoder_rnn(input_)
    all_=all_.view(-1,self.hidden_size)
    #all_ [word_len,hidden_size] final[1,1,hidden_size]

    output=[]
    attention_record=[]
    decoder_hidden=final
    decoder_input=torch.zeros(1,1,self.output_size).to(device)
    vector2=self.W(all_)
    #vector2 [word_len,hidden_size]
    for i in range(max_output):
      #Attention
      #decoder_hidden.view(1,-1) [1,hidden_size]
      vector1=self.U(decoder_hidden.view(1,-1))
      attention=self.atten(torch.tanh(vector1+vector2))
      attn_weights=fn.softmax(attention.view(1,-1),dim=1)
      attention_record.append(attn_weights.tolist())
      #attn_weights [1,1,word_len]
      #all_ [1,word_len,hidden_size]
      attentioned=torch.bmm(attn_weights.unsqueeze(0),all_.unsqueeze(0))
      #attentioned [1,1,hidden_size]
      embed=self.embedding(decoder_input)
      #attentioned [1,1,hidden_size]
      #decoder_input=torch.cat((embed[0],attentioned[0]),1).unsqueeze(0)
      decoder_input=torch.cat((embed,attentioned),2)
      #Decoder RNN
      out,decoder_hidden=self.decoder_rnn(decoder_input,decoder_hidden)
      
      out=self.h2o(decoder_hidden)
      out=self.sm(out)
      #out [1,1,output_size]
      output.append(out.view(1,-1))

      max_id=torch.argmax(out,2,keepdim=True)
      if assisted is not None:
        max_id=assisted[i].view(1,1,-1)
      hot=torch.zeros(1,1,self.output_size,device=device)
      hot.scatter_(2,max_id,1)

      decoder_input=hot.detach()

    return output,attention_record

english_ascii_range=[97,122]
hindi_ascii_range=[2304,2432]
english_letters=letters("english")
english_letters.fill(english_ascii_range)
hindi_letters=letters("hindi")
hindi_letters.fill(hindi_ascii_range)

model=TransliterationAttention(english_letters.letter_count,512,hindi_letters.letter_count)
model.load_state_dict(torch.load("transliteration_model.pt"), strict=False)

model.eval()
word=input("Enter a english word : ")
word1_tensor=word_tensors(word,english_letters)
output,attention=model.forward(word1_tensor)
op=""
final=[]
print("Hindi conversion : ",end="")
for i in output:
  if i.argmax(1).item()==EOS:
    continue
  op+=hindi_letters.index2letter[i.argmax(1).item()]
print(op)
